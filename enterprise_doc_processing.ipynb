{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 企业文档管理的三大痛点\n",
        "\n",
        "企业内部文档散落在各处:技术文档是PDF,产品手册是Word,会议记录是Markdown,知识沉淀在网页中。\n",
        "\n",
        "本文展示如何使用代码构建统一文档处理流水线。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Caching the list of root modules, please wait!\n",
            "(This will only be done once - type '%rehashx' to reset cache!)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# 设置文档目录\n",
        "DOCS_DIR = \"./files/enterprise_docs\"\n",
        "OUTPUT_DIR = \"./files/processed_docs\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "# 2:实现多格式文档加载器\n",
        "from typing import List\n",
        "from langchain_classic.schema import Document\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    WebBaseLoader\n",
        ")\n",
        "\n",
        "class MultiFormatDocumentLoader:\n",
        "    \"\"\"多格式文档统一加载器\"\"\"\n",
        "    \n",
        "    def __init__(self, docs_dir: str):\n",
        "        self.docs_dir = docs_dir\n",
        "        self.supported_formats = {\n",
        "            '.pdf': self._load_pdf,\n",
        "            '.md': self._load_markdown,\n",
        "            '.html': self._load_web\n",
        "        }\n",
        "    \n",
        "    def _load_pdf(self, file_path: str) -> List[Document]:\n",
        "        \"\"\"加载PDF文档\"\"\"\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents = loader.load()\n",
        "        # 添加元数据\n",
        "        for doc in documents:\n",
        "            doc.metadata['source_type'] = 'pdf'\n",
        "            doc.metadata['file_path'] = file_path\n",
        "        return documents\n",
        "    \n",
        "    def _load_markdown(self, file_path: str) -> List[Document]:\n",
        "        \"\"\"加载Markdown文档\"\"\"\n",
        "        loader = UnstructuredMarkdownLoader(file_path)\n",
        "        documents = loader.load()\n",
        "        for doc in documents:\n",
        "            doc.metadata['source_type'] = 'markdown'\n",
        "            doc.metadata['file_path'] = file_path\n",
        "        return documents\n",
        "    \n",
        "    def _load_web(self, url: str) -> List[Document]:\n",
        "        \"\"\"加载网页内容\"\"\"\n",
        "        loader = WebBaseLoader(url)\n",
        "        documents = loader.load()\n",
        "        for doc in documents:\n",
        "            doc.metadata['source_type'] = 'web'\n",
        "            doc.metadata['url'] = url\n",
        "        return documents\n",
        "    \n",
        "    def load_all(self) -> List[Document]:\n",
        "        \"\"\"加载目录下所有支持格式的文档\"\"\"\n",
        "        all_documents = []\n",
        "        \n",
        "        for filename in os.listdir(self.docs_dir):\n",
        "            file_path = os.path.join(self.docs_dir, filename)\n",
        "            file_ext = os.path.splitext(filename)[1].lower()\n",
        "            \n",
        "            if file_ext in self.supported_formats:\n",
        "                try:\n",
        "                    loader_func = self.supported_formats[file_ext]\n",
        "                    documents = loader_func(file_path)\n",
        "                    all_documents.extend(documents)\n",
        "                    print(f\"✅ 已加载: {filename} ({len(documents)}个文档片段)\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ 加载失败 {filename}: {str(e)}\")\n",
        "        \n",
        "        return all_documents\n",
        "    \n",
        "    def load_from_urls(self, urls: List[str]) -> List[Document]:\n",
        "        \"\"\"从URL列表加载网页\"\"\"\n",
        "        documents = []\n",
        "        for url in urls:\n",
        "            try:\n",
        "                docs = self._load_web(url)\n",
        "                documents.extend(docs)\n",
        "                print(f\"✅ 已加载网页: {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 加载网页失败 {url}: {str(e)}\")\n",
        "        return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 步骤3: 文本清洗\n",
        "import re\n",
        "from typing import List\n",
        "from langchain_classic.schema import Document\n",
        "\n",
        "class TextCleaner:\n",
        "    \"\"\"文本清洗工具类\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_extra_whitespace(text: str) -> str:\n",
        "        \"\"\"去除多余空白字符\"\"\"\n",
        "        # 多个空格→一个空格\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # 多个连续换行→两个换行\n",
        "        text = re.sub(r'\\n+', '\\n\\n', text)\n",
        "        # 去除每行行首尾空格\n",
        "        text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
        "        return text.strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_special_chars(text: str) -> str:\n",
        "        \"\"\"去除特殊字符(保留中文、英文、数字和常用标点)\"\"\"\n",
        "        # ⚠ 方括号需转义 \\[ \\]\n",
        "        # ⚠ 单双引号用 \\\" \\' 避免破坏字符串\n",
        "        pattern = (\n",
        "            r'[^\\u4e00-\\u9fa5a-zA-Z0-9\\s'\n",
        "            r'\\.\\,\\!\\?\\;\\:\\(\\)\\[\\]\\{\\}《》\\\"\\'、。]'\n",
        "        )\n",
        "        return re.sub(pattern, '', text)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_chinese_punctuation(text: str) -> str:\n",
        "        \"\"\"统一中文标点符号（可按需扩展）\"\"\"\n",
        "        replacements = {\n",
        "            ',': '、',\n",
        "            '(': '（',\n",
        "            ')': '）',\n",
        "            '[': '【',\n",
        "            ']': '】'\n",
        "        }\n",
        "        for a, b in replacements.items():\n",
        "            text = text.replace(a, b)\n",
        "        return text\n",
        "\n",
        "    @classmethod\n",
        "    def clean(cls, text: str, normalize_punctuation: bool = True) -> str:\n",
        "        \"\"\"执行完整清洗流程\"\"\"\n",
        "        text = cls.remove_extra_whitespace(text)\n",
        "        text = cls.remove_special_chars(text)\n",
        "        if normalize_punctuation:\n",
        "            text = cls.normalize_chinese_punctuation(text)\n",
        "        return text\n",
        "\n",
        "\n",
        "def clean_documents(documents: List[Document]) -> List[Document]:\n",
        "    \"\"\"批量清洗文档\"\"\"\n",
        "    cleaned_docs = []\n",
        "    for doc in documents:\n",
        "        cleaned = TextCleaner.clean(doc.page_content)\n",
        "        if len(cleaned.strip()) > 50:  # 至少保留50字符\n",
        "            cleaned_docs.append(Document(\n",
        "                page_content=cleaned,\n",
        "                metadata=doc.metadata\n",
        "            ))\n",
        "    print(f\"✅ 清洗完成: {len(documents)}篇 → {len(cleaned_docs)}篇有效文档\")\n",
        "    return cleaned_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 步骤4: 语义分块\n",
        "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_classic.schema import Document\n",
        "from typing import List\n",
        "\n",
        "class SemanticTextSplitter:\n",
        "    \"\"\"语义文本分块器\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        chunk_size: int = 1000,\n",
        "        chunk_overlap: int = 200\n",
        "    ):\n",
        "        \"\"\"\n",
        "        初始化分块器\n",
        "\n",
        "        Args:\n",
        "            chunk_size: 每个文本块的目标大小(字符数)\n",
        "            chunk_overlap: 相邻块之间的重叠部分\n",
        "        \"\"\"\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\n",
        "                \"\\n\\n\",  # 段落\n",
        "                \"\\n\",    # 换行\n",
        "                \"。\",     # 中文句号\n",
        "                \".\",      # 英文句号\n",
        "                \";\",      # 分号\n",
        "                \",\",      # 逗号\n",
        "                \" \",      # 空格\n",
        "                \"\"        # 字符\n",
        "            ]\n",
        "        )\n",
        "    \n",
        "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
        "        \"\"\"分块文档列表\"\"\"\n",
        "        chunks = self.splitter.split_documents(documents)\n",
        "        \n",
        "        # 为每个 chunk 添加块索引和大小\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk.metadata['chunk_id'] = i\n",
        "            chunk.metadata['chunk_size'] = len(chunk.page_content)\n",
        "        \n",
        "        print(f\"✅ 分块完成: {len(documents)}篇文档 → {len(chunks)}个文本块\")\n",
        "        return chunks\n",
        "    \n",
        "    def get_statistics(self, chunks: List[Document]) -> dict:\n",
        "        \"\"\"获取分块统计信息\"\"\"\n",
        "        sizes = [len(chunk.page_content) for chunk in chunks]\n",
        "        return {\n",
        "            'total_chunks': len(chunks),\n",
        "            'avg_size': sum(sizes) / len(sizes) if sizes else 0,\n",
        "            'min_size': min(sizes) if sizes else 0,\n",
        "            'max_size': max(sizes) if sizes else 0\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "企业知识库文档处理流程\n",
            "============================================================\n",
            "\n",
            "[步骤1] 加载文档...\n",
            "✅ 已加载网页: https://cn.lipsum.com/\n",
            "共加载 1 个文档片段\n",
            "\n",
            "[步骤2] 清洗文档...\n",
            "✅ 清洗完成: 1篇 → 1篇有效文档\n",
            "\n",
            "[步骤3] 分块文档...\n",
            "✅ 分块完成: 1篇文档 → 9个文本块\n",
            "\n",
            "[统计信息]\n",
            "  - 总文本块数: 9\n",
            "  - 平均块大小: 726 字符\n",
            "  - 最小块大小: 65 字符\n",
            "  - 最大块大小: 998 字符\n",
            "\n",
            "[示例文本块]\n",
            "来源: web\n",
            "内容预览: Lorem Ipsum  All the facts  Lipsum generator\n",
            "\n",
            " Shqip   Catal 中文简体 Hrvatski esky Dansk Nederlands English Eesti Filipino Suomi Franais  Deutsch    Magyar Indonesia Italiano Latviski Lietuvikai  Melayu  ...\n"
          ]
        }
      ],
      "source": [
        "def process_enterprise_documents(\n",
        "    docs_dir: str,\n",
        "    web_urls: List[str] = None,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 200\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    企业文档处理完整流程\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"企业知识库文档处理流程\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 1. 加载文档\n",
        "    print(\"\\n[步骤1] 加载文档...\")\n",
        "    loader = MultiFormatDocumentLoader(docs_dir)\n",
        "    documents = loader.load_all()\n",
        "    \n",
        "    if web_urls:\n",
        "        web_docs = loader.load_from_urls(web_urls)\n",
        "        documents.extend(web_docs)\n",
        "    \n",
        "    print(f\"共加载 {len(documents)} 个文档片段\")\n",
        "    \n",
        "    # 2. 清洗文档\n",
        "    print(\"\\n[步骤2] 清洗文档...\")\n",
        "    cleaned_docs = clean_documents(documents)\n",
        "    \n",
        "    # 3. 分块文档\n",
        "    print(\"\\n[步骤3] 分块文档...\")\n",
        "    splitter = SemanticTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = splitter.split_documents(cleaned_docs)\n",
        "    \n",
        "    # 4. 统计信息\n",
        "    stats = splitter.get_statistics(chunks)\n",
        "    print(\"\\n[统计信息]\")\n",
        "    print(f\"  - 总文本块数: {stats['total_chunks']}\")\n",
        "    print(f\"  - 平均块大小: {stats['avg_size']:.0f} 字符\")\n",
        "    print(f\"  - 最小块大小: {stats['min_size']} 字符\")\n",
        "    print(f\"  - 最大块大小: {stats['max_size']} 字符\")\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "#                    示例运行部分\n",
        "# ======================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    web_urls = [\n",
        "        \"https://cn.lipsum.com/\",\n",
        "    ]\n",
        "    \n",
        "    chunks = process_enterprise_documents(\n",
        "        docs_dir=\"./files/enterprise_docs\",\n",
        "        web_urls=web_urls,\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    \n",
        "    print(\"\\n[示例文本块]\")\n",
        "    print(\"来源:\", chunks[0].metadata.get(\"source_type\", \"unknown\"))\n",
        "    print(\"内容预览:\", chunks[0].page_content[:200], \"...\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
