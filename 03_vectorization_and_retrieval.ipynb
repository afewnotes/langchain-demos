{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量化与高效检索实战\n",
    "\n",
    "本notebook演示如何构建可检索的向量知识库，包括文档向量化、相似度检索等核心功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 文档加载与分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载了 613 页文档\n"
     ]
    }
   ],
   "source": [
    "# 加载PDF文档\n",
    "pdf_loader = PyPDFLoader(\"files/ddia.pdf\")\n",
    "raw_docs = pdf_loader.load()\n",
    "\n",
    "print(f\"加载了 {len(raw_docs)} 页文档\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割为 2340 个文本块\n",
      "\n",
      "示例块：\n",
      "Martin Kleppmann\n",
      "Designing \n",
      "Data-Intensive \n",
      "Applications\n",
      "THE BIG IDEAS BEHIND RELIABLE, SCALABLE,  \n",
      "AND MAINTAINABLE SYSTEMS...\n"
     ]
    }
   ],
   "source": [
    "# 文档分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "text_chunks = text_splitter.split_documents(raw_docs)\n",
    "print(f\"分割为 {len(text_chunks)} 个文本块\")\n",
    "print(f\"\\n示例块：\\n{text_chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding模型初始化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量维度：2560\n",
      "向量示例（前5维）：[-0.0004277588741388172, -0.03691500797867775, 0.020067891106009483, 0.031092843040823936, -0.0027407535817474127]\n"
     ]
    }
   ],
   "source": [
    "# 初始化Embedding模型\n",
    "# embedding_model = OpenAIEmbeddings(\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# )\n",
    "from langchain.agents import create_agent\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "# 其他供应商\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"Qwen/Qwen3-Embedding-4B\"\n",
    ")\n",
    "\n",
    "# 测试向量化\n",
    "test_vector = embedding_model.embed_query(\"深度学习\")\n",
    "print(f\"向量维度：{len(test_vector)}\")\n",
    "print(f\"向量示例（前5维）：{test_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 构建向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量数据库已创建，包含 2340 条记录\n"
     ]
    }
   ],
   "source": [
    "# 创建Chroma向量数据库\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./files/vector_storage\"\n",
    ")\n",
    "\n",
    "print(f\"向量数据库已创建，包含 {vector_db._collection.count()} 条记录\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 相似度检索\n",
    "\n",
    "使用余弦相似度找到与查询最相关的文档块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询：what is data intensive？\n",
      "\n",
      "检索到 3 个相关文档：\n",
      "\n",
      "--- 结果 1 ---\n",
      "来源：第 512 页\n",
      "内容：usefulness of a technology. The range of different things you might want to do with\n",
      "data is dizzyingly wide. What one person considers to be an obscur...\n",
      "\n",
      "--- 结果 2 ---\n",
      "来源：第 16 页\n",
      "内容：the job, and different technologies each have their own strengths and weaknesses. As\n",
      "we shall see, relational databases are important but not the fina...\n",
      "\n",
      "--- 结果 3 ---\n",
      "来源：第 276 页\n",
      "内容：current date and time, for example, it must do so through special deterministic APIs. \n",
      "Partitioning\n",
      "Executing all transactions serially makes concurre...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 执行相似度检索\n",
    "query_text = \"what is data intensive？\"\n",
    "\n",
    "retrieved_docs = vector_db.similarity_search(\n",
    "    query_text,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(f\"查询：{query_text}\\n\")\n",
    "print(f\"检索到 {len(retrieved_docs)} 个相关文档：\\n\")\n",
    "\n",
    "for idx, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"--- 结果 {idx} ---\")\n",
    "    print(f\"来源：第 {doc.metadata.get('page', 'N/A')} 页\")\n",
    "    print(f\"内容：{doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 带相似度评分的检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索结果及相似度评分：\n",
      "\n",
      "结果 1 - 距离分数：0.8561\n",
      "内容：usefulness of a technology. The range of different things you might want to do with\n",
      "data is dizzying...\n",
      "\n",
      "结果 2 - 距离分数：0.8584\n",
      "内容：the job, and different technologies each have their own strengths and weaknesses. As\n",
      "we shall see, r...\n",
      "\n",
      "结果 3 - 距离分数：0.8616\n",
      "内容：current date and time, for example, it must do so through special deterministic APIs. \n",
      "Partitioning\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 获取带评分的检索结果\n",
    "docs_with_scores = vector_db.similarity_search_with_score(\n",
    "    query_text,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"检索结果及相似度评分：\\n\")\n",
    "for idx, (doc, score) in enumerate(docs_with_scores, 1):\n",
    "    print(f\"结果 {idx} - 距离分数：{score:.4f}\")\n",
    "    print(f\"内容：{doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 使用检索器接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到 3 个文档\n"
     ]
    }
   ],
   "source": [
    "# 创建检索器\n",
    "retriever = vector_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# 使用检索器\n",
    "results = retriever.invoke(query_text)\n",
    "print(f\"检索到 {len(results)} 个文档\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 混合检索（语义+关键词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混合检索到 5 个文档\n",
      "Doc 1: page_content='usefulness of a technology. The range of different things you might want to do with\n",
      "data is dizzyingly wide. What one person considers to be an obscure and pointless\n",
      "feature may well be \n",
      "Doc 2: page_content='idea of using multiple differently partitioned stages is similar to what we discussed in\n",
      "“Multi-partition data processing” on page 514 (see also “Concurrency control” on\n",
      "page 462). \n",
      "Time\n",
      "Doc 3: page_content='the job, and different technologies each have their own strengths and weaknesses. As\n",
      "we shall see, relational databases are important but not the final word on dealing with\n",
      "data.\n",
      "Scope o\n",
      "Doc 4: page_content='understanding.\n",
      "Moreover, data is extracted from users through a one-way process, not a relationship\n",
      "with true reciprocity, and not a fair value exchange. There is no dialog, no option fo\n",
      "Doc 5: page_content='current date and time, for example, it must do so through special deterministic APIs. \n",
      "Partitioning\n",
      "Executing all transactions serially makes concurrency control much simpler, but lim‐\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# -------------------------\n",
    "# 1. 定义自定义混合检索器\n",
    "# -------------------------\n",
    "class HybridRetriever:\n",
    "    \"\"\"\n",
    "    支持多个检索器混合 + 权重 + 去重\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, retrievers, weights=None, k=5):\n",
    "        self.retrievers = retrievers\n",
    "        self.weights = weights or [1] * len(retrievers)\n",
    "        self.k = k\n",
    "\n",
    "    def invoke(self, query: str) -> List:\n",
    "        scored_docs = {}\n",
    "\n",
    "        for retriever, weight in zip(self.retrievers, self.weights):\n",
    "            # 新版 retriever 用 invoke()\n",
    "            docs = retriever.invoke(query)\n",
    "\n",
    "            for rank, doc in enumerate(docs):\n",
    "                # 兼容新版，不一定有 page_content\n",
    "                content = getattr(doc, \"page_content\", str(doc))\n",
    "                score = weight * (1 / (rank + 1))\n",
    "\n",
    "                if content not in scored_docs:\n",
    "                    scored_docs[content] = (doc, score)\n",
    "                else:\n",
    "                    scored_docs[content] = (doc, scored_docs[content][1] + score)\n",
    "\n",
    "        # 按分数排序\n",
    "        sorted_docs = sorted(scored_docs.values(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 返回前 k 条文档\n",
    "        return [doc for doc, _ in sorted_docs[:self.k]]\n",
    "\n",
    "# -------------------------\n",
    "# 2. 初始化各个检索器\n",
    "# -------------------------\n",
    "# BM25 检索器\n",
    "keyword_retriever = BM25Retriever.from_documents(text_chunks)\n",
    "keyword_retriever.k = 3\n",
    "\n",
    "# 向量检索器\n",
    "vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# -------------------------\n",
    "# 3. 创建混合检索器\n",
    "# -------------------------\n",
    "hybrid_retriever = HybridRetriever(\n",
    "    retrievers=[keyword_retriever, vector_retriever],\n",
    "    weights=[0.4, 0.6],  # BM25 40%，向量 60%\n",
    "    k=5\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4. 执行检索\n",
    "# -------------------------\n",
    "hybrid_results = hybrid_retriever.invoke(query_text)\n",
    "\n",
    "print(f\"混合检索到 {len(hybrid_results)} 个文档\")\n",
    "\n",
    "for i, doc in enumerate(hybrid_results):\n",
    "    print(f\"Doc {i+1}: {str(doc)[:200]}\")  # 取前200字符展示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. MMR检索（最大边际相关性）\n",
    "\n",
    "MMR在保证相关性的同时增加结果多样性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR检索结果：\n",
      "\n",
      "结果 1：usefulness of a technology. The range of different things you might want to do with\n",
      "data is dizzying...\n",
      "\n",
      "结果 2：that requests from a particular user are always routed to the same datacenter and use\n",
      "the leader in ...\n",
      "\n",
      "结果 3：that it helps preserve causality. We have already seen several examples over the\n",
      "course of this book...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用MMR检索\n",
    "mmr_docs = vector_db.max_marginal_relevance_search(\n",
    "    query_text,\n",
    "    k=3,\n",
    "    fetch_k=10  # 先获取10个候选，再筛选3个\n",
    ")\n",
    "\n",
    "print(\"MMR检索结果：\\n\")\n",
    "for idx, doc in enumerate(mmr_docs, 1):\n",
    "    print(f\"结果 {idx}：{doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 持久化与增量更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载已有数据库，包含 2340 条记录\n"
     ]
    }
   ],
   "source": [
    "# 加载已有数据库\n",
    "existing_db = Chroma(\n",
    "    persist_directory=\"./files/vector_storage\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "print(f\"加载已有数据库，包含 {existing_db._collection.count()} 条记录\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加后数据库包含 2342 条记录\n"
     ]
    }
   ],
   "source": [
    "# 增量添加新文档\n",
    "new_texts = [\n",
    "    \"Transformer架构是现代NLP的基础\",\n",
    "    \"注意力机制能够捕捉长距离依赖关系\"\n",
    "]\n",
    "\n",
    "existing_db.add_texts(new_texts)\n",
    "print(f\"添加后数据库包含 {existing_db._collection.count()} 条记录\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 元数据过滤检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤检索到 3 个文档\n"
     ]
    }
   ],
   "source": [
    "# 带元数据过滤的检索\n",
    "filtered_docs = vector_db.similarity_search(\n",
    "    query_text,\n",
    "    k=3,\n",
    "    filter={\"page\": {\"$gte\": 5}}  # 只检索第5页及之后的内容\n",
    ")\n",
    "\n",
    "print(f\"过滤检索到 {len(filtered_docs)} 个文档\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 批量处理示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理 50/2340 个文档\n",
      "已处理 100/2340 个文档\n"
     ]
    }
   ],
   "source": [
    "# 批量向量化大规模文档\n",
    "def batch_process_documents(chunks, batch_size=50):\n",
    "    \"\"\"分批处理文档以避免超时\"\"\"\n",
    "    for i in range(0, 100, batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        vector_db.add_documents(batch)\n",
    "        processed = min(i+batch_size, len(chunks))\n",
    "        print(f\"已处理 {processed}/{len(chunks)} 个文档\")\n",
    "\n",
    "# 示例调用\n",
    "batch_process_documents(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本notebook演示了：\n",
    "- 文档向量化的完整流程\n",
    "- 多种检索策略（相似度、MMR、混合检索）\n",
    "- 向量数据库的持久化和增量更新\n",
    "\n",
    "下一步可以将这些组件整合到完整的RAG应用中。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
